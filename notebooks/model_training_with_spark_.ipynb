{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f7456a4-c3ea-4b94-8e76-b91b9d72df3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Obtaining dependency information for xgboost from https://files.pythonhosted.org/packages/7e/67/499fbd8b5b3a8ee2196463567acff913491f10268f9f67f51fe3370e80e2/xgboost-1.7.6-py3-none-manylinux2014_aarch64.whl.metadata\n",
      "  Downloading xgboost-1.7.6-py3-none-manylinux2014_aarch64.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from xgboost) (1.24.4)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.11/site-packages (from xgboost) (1.11.1)\n",
      "Downloading xgboost-1.7.6-py3-none-manylinux2014_aarch64.whl (2.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hInstalling collected packages: xgboost\n",
      "Successfully installed xgboost-1.7.6\n",
      "Collecting findspark\n",
      "  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
      "Installing collected packages: findspark\n",
      "Successfully installed findspark-2.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost\n",
    "!pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "677d7d59-2932-4850-b751-2fc1fe912a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/work\n"
     ]
    }
   ],
   "source": [
    "cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "084524e4-7ff7-41a3-90db-45ede78b03b1",
   "metadata": {
    "id": "084524e4-7ff7-41a3-90db-45ede78b03b1"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, OneHotEncoderModel\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from pyspark.sql.functions import col, isnan, when, count\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "import os\n",
    "import shutil\n",
    "import pickle\n",
    "import  numpy as np\n",
    "from pyspark.ml.clustering import KMeans,KMeansModel\n",
    "from pyspark.ml.feature import Imputer, ImputerModel\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.feature import StandardScaler, StandardScalerModel\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from xgboost.spark import SparkXGBRegressor\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84d91df5-19be-44ca-aba4-6f06a57df45b",
   "metadata": {
    "id": "84d91df5-19be-44ca-aba4-6f06a57df45b"
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"store_sales_prediction\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b72e0cb-8ac7-4180-ada9-fcccc8088878",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'preprocessing_models/'\n",
    "if not os.path.isdir(path):\n",
    "    os.mkdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39b9d7e0-b39f-40e5-93fb-3701aad9c4d2",
   "metadata": {
    "id": "39b9d7e0-b39f-40e5-93fb-3701aad9c4d2"
   },
   "outputs": [],
   "source": [
    "def split_dataset(path):\n",
    "  # Load the data into a Spark DataFrame\n",
    "  df = spark.read.csv(path, header=True, inferSchema=True)\n",
    "\n",
    "  # Split the data into training and testing sets\n",
    "  train_df, test_df = df.randomSplit([0.8, 0.2], seed=100)\n",
    "  return train_df,test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67a98e4f-bf74-4b12-8745-c5d03d9b118e",
   "metadata": {
    "id": "67a98e4f-bf74-4b12-8745-c5d03d9b118e"
   },
   "outputs": [],
   "source": [
    "def concat_dataframe(df1, df2):\n",
    "    # Add index column to x_train\n",
    "    df1 = df1.withColumn(\"index\", monotonically_increasing_id())\n",
    "\n",
    "    # Add index column to y_train\n",
    "    df2 = df2.withColumn(\"index\", monotonically_increasing_id())\n",
    "\n",
    "    # Perform an inner join on the index column\n",
    "    df = df1.join(df2, \"index\").drop(\"index\")\n",
    "    return df\n",
    "\n",
    "def modify_columns(df, col_names, extension):\n",
    "    df = df.drop(*col_names)\n",
    "    for col in col_names:\n",
    "        df = df.withColumnRenamed(col + extension, col)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1643e9f9-7a26-423b-b5af-803cc92e4231",
   "metadata": {
    "id": "1643e9f9-7a26-423b-b5af-803cc92e4231"
   },
   "outputs": [],
   "source": [
    "def cluster_data(data, train=True):\n",
    "    # Prepare features using VectorAssembler\n",
    "    feature_cols = [col for col in data.columns if col != 'Item_Outlet_Sales']\n",
    "    vector_assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "    data_with_features = vector_assembler.transform(data)\n",
    "    if train:\n",
    "      silhouette_scores = {}\n",
    "      for i in range(2,5):  # Start from k=2 instead of k=1\n",
    "          kmeans = KMeans().setK(i).setSeed(1)\n",
    "          model = kmeans.fit(data_with_features.repartition(i))\n",
    "\n",
    "          # Compute the Silhouette score\n",
    "          evaluator = ClusteringEvaluator()\n",
    "          predictions = model.transform(data_with_features)\n",
    "          silhouette_score = evaluator.evaluate(predictions)\n",
    "          silhouette_scores[silhouette_score]=i\n",
    "      optimal_cluster = silhouette_scores[max(silhouette_scores.keys())]\n",
    "      cluster_model = KMeans().setK(optimal_cluster).setSeed(1)\n",
    "      cluster_model = cluster_model.fit(data_with_features.repartition(optimal_cluster))\n",
    "      cluster_model.write().overwrite().save(path+'cluster_model')\n",
    "    else:\n",
    "      cluster_model = KMeansModel.load(path+'cluster_model')\n",
    "    data_with_clusters = cluster_model.transform(data_with_features)\n",
    "\n",
    "\n",
    "    data_with_clusters = data_with_clusters.withColumnRenamed('prediction', 'Cluster')\n",
    "    data = data_with_clusters.drop(*feature_cols)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c16e24c6-c6c9-4f30-847b-59d8a6ca89a5",
   "metadata": {
    "id": "c16e24c6-c6c9-4f30-847b-59d8a6ca89a5"
   },
   "outputs": [],
   "source": [
    "def preprocessing_pipeline(data, train=True):\n",
    "    # data = concat_dataframe(x_train, y_train)\n",
    "\n",
    "\n",
    "    # Dropping unnecessary column\n",
    "    data = data.drop('Item_Type')\n",
    "\n",
    "    ### Editing dataset\n",
    "    data = data.withColumn('Item_Visibility', F.when(data['Item_Visibility'] == 0, None).otherwise(data['Item_Visibility']))\n",
    "\n",
    "    data = data.withColumn('Item_Fat_Content', F.when(\n",
    "        (data['Item_Fat_Content'] == 'low fat') | (data['Item_Fat_Content'] == 'LF'), 'Low Fat'\n",
    "    ).otherwise(F.when(data['Item_Fat_Content'] == 'reg', 'Regular').otherwise(data['Item_Fat_Content'])))\n",
    "\n",
    "    # Apply lambda function on 'Item_Identifier'\n",
    "    item_identifier_udf = F.udf(lambda x: x[:2], F.StringType())\n",
    "    data = data.withColumn('Item_Identifier', item_identifier_udf(data['Item_Identifier']))\n",
    "\n",
    "    # Calculate Outlet_Age and drop Outlet_Establishment_Year\n",
    "    data = data.withColumn('Outlet_Age', (F.lit(2013) - data['Outlet_Establishment_Year']).cast(IntegerType()))\n",
    "    data = data.drop('Outlet_Establishment_Year')\n",
    "\n",
    "    # Update 'Item_Fat_Content' for Item_Identifier==\"NC\"\n",
    "    data = data.withColumn('Item_Fat_Content', F.when(data['Item_Identifier'] == \"NC\", 'Non Edible').otherwise(data['Item_Fat_Content']))\n",
    "    # Encoding categorical values\n",
    "    data = data.withColumn('Outlet_Size', F.when(data['Outlet_Size'] == 'Small', 0)\n",
    "                         .when(data['Outlet_Size'] == 'Medium', 1)\n",
    "                         .when(data['Outlet_Size'] == 'High', 2)\n",
    "                         .otherwise(None))\n",
    "    all_columns=data.columns\n",
    "    # Identify the categorical and numeric columns\n",
    "    categorical_cols = ['Item_Identifier', 'Item_Fat_Content', 'Outlet_Identifier', 'Outlet_Type', 'Outlet_Location_Type']\n",
    "    numeric_cols = [col for col in all_columns if col not in categorical_cols and col != 'Item_Outlet_Sales']\n",
    "\n",
    "    ### Index the categorical columns\n",
    "    if train:\n",
    "        indexers = [StringIndexer(inputCol=col, outputCol=col+'_index', handleInvalid='skip') for col in categorical_cols]\n",
    "        indexer_pipeline = Pipeline(stages=indexers)\n",
    "        indexer_model = indexer_pipeline.fit(data)\n",
    "        indexer_model.write().overwrite().save(path+'indexer_model')\n",
    "    else:\n",
    "        indexer_model = PipelineModel.load(path+'indexer_model')\n",
    "    data = indexer_model.transform(data)\n",
    "    data = modify_columns(data, categorical_cols, '_index')\n",
    "\n",
    "\n",
    "    ### Impute missing values in all columns\n",
    "    imputed_cols = [i for i in data.columns if i!='Item_Outlet_Sales']\n",
    "    if train:\n",
    "        imputer = Imputer(inputCols=imputed_cols, outputCols=[col + '_imputed' for col in imputed_cols], strategy='mean')\n",
    "        imputer_model = imputer.fit(data)\n",
    "        imputer_model.write().overwrite().save(path+'imputer_model')\n",
    "    else:\n",
    "        imputer_model = ImputerModel.load(path+'imputer_model')\n",
    "    data = imputer_model.transform(data)\n",
    "    data = modify_columns(data, imputed_cols, '_imputed')\n",
    "\n",
    "\n",
    "\n",
    "    ###onehot encodding\n",
    "    if train:\n",
    "        encoder = OneHotEncoder(inputCols=categorical_cols, outputCols=[col + '_encoded' for col in categorical_cols], dropLast=False)\n",
    "        encoder_model = encoder.fit(data)\n",
    "        encoder_model.write().overwrite().save(path+'encoder_model')\n",
    "    else:\n",
    "        encoder_model = OneHotEncoderModel.load(path+'encoder_model')\n",
    "    data = encoder_model.transform(data)\n",
    "    data = modify_columns(data, categorical_cols, '_encoded')\n",
    "    #scaling numerical cols\n",
    "    scaling_cols=['Item_Weight', 'Item_Visibility', 'Item_MRP', 'Outlet_Age']\n",
    "    assembler = VectorAssembler(inputCols=scaling_cols, outputCol=\"features\")\n",
    "    data = assembler.transform(data)\n",
    "    data = data.drop(*scaling_cols)\n",
    "    if train:\n",
    "      scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withStd=True, withMean=True)\n",
    "      scaler_model = scaler.fit(data)\n",
    "      scaler_model.write().overwrite().save(path+'scaler_model')\n",
    "    else:\n",
    "      scaler_model = StandardScalerModel.load(path+'scaler_model')\n",
    "\n",
    "    # Transform the numerical columns using the scaler\n",
    "    data = scaler_model.transform(data)\n",
    "    data = data.drop('features')\n",
    "\n",
    "\n",
    "    if train:\n",
    "      data = cluster_data(data, train=True)\n",
    "    else:\n",
    "      data = cluster_data(data, train=False)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70efef79-3f1c-4e73-8603-d094f079b43b",
   "metadata": {
    "id": "70efef79-3f1c-4e73-8603-d094f079b43b"
   },
   "outputs": [],
   "source": [
    "def get_best_param_xgb(train_df, test_df):\n",
    "    # Remove the target column from the input feature set.\n",
    "    featuresCols = train_df.columns\n",
    "    featuresCols.remove('Item_Outlet_Sales')\n",
    "\n",
    "    # vectorAssembler combines all feature columns into a single feature vector column, \"rawFeatures\".\n",
    "    vectorAssembler = VectorAssembler(inputCols=featuresCols, outputCol=\"features\")\n",
    "    xgb_regressor = SparkXGBRegressor(num_workers=2, label_col=\"Item_Outlet_Sales\", missing=0.0)\n",
    "\n",
    "    paramGrid = ParamGridBuilder()\\\n",
    "      .addGrid(xgb_regressor.max_depth, [2, 5])\\\n",
    "      .addGrid(xgb_regressor.n_estimators, [10, 100])\\\n",
    "      .build()\n",
    "\n",
    "    # Define an evaluation metric.  The CrossValidator compares the true labels with predicted values for each combination of parameters, and calculates this value to determine the best model.\n",
    "    evaluator = RegressionEvaluator(metricName=\"r2\",\n",
    "                                    labelCol=xgb_regressor.getLabelCol(),\n",
    "                                    predictionCol=xgb_regressor.getPredictionCol())\n",
    "\n",
    "    # Declare the CrossValidator, which performs the model tuning.\n",
    "    cv = CrossValidator(estimator=xgb_regressor, evaluator=evaluator, estimatorParamMaps=paramGrid)\n",
    "\n",
    "    pipeline = Pipeline(stages=[vectorAssembler, cv])\n",
    "    pipelineModel = pipeline.fit(train_df)\n",
    "    predictions = pipelineModel.transform(test_df)\n",
    "    r2 = evaluator.evaluate(predictions)\n",
    "\n",
    "    return pipelineModel, r2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d6e8d98-0cd8-49a4-9a04-17db0a8f2387",
   "metadata": {
    "id": "7d6e8d98-0cd8-49a4-9a04-17db0a8f2387"
   },
   "outputs": [],
   "source": [
    "def get_best_param_rf(train_df, test_df):\n",
    "    # Remove the target column from the input feature set.\n",
    "    featuresCols = train_df.columns\n",
    "    featuresCols.remove('Item_Outlet_Sales')\n",
    "\n",
    "    # vectorAssembler combines all feature columns into a single feature vector column, \"rawFeatures\".\n",
    "    vectorAssembler = VectorAssembler(inputCols=featuresCols, outputCol=\"features\")\n",
    "    rf = RandomForestRegressor(labelCol='Item_Outlet_Sales')\n",
    "\n",
    "    paramGrid = ParamGridBuilder() \\\n",
    "        .addGrid(rf.numTrees, [10,20]) \\\n",
    "        .addGrid(rf.maxDepth, [5, 10]) \\\n",
    "        .build()\n",
    "\n",
    "    # Define an evaluation metric.  The CrossValidator compares the true labels with predicted values for each combination of parameters, and calculates this value to determine the best model.\n",
    "    evaluator = RegressionEvaluator(metricName=\"r2\",\n",
    "                                    labelCol=rf.getLabelCol(),\n",
    "                                    predictionCol=rf.getPredictionCol())\n",
    "\n",
    "    # Declare the CrossValidator, which performs the model tuning.\n",
    "    cv = CrossValidator(estimator=rf, evaluator=evaluator, estimatorParamMaps=paramGrid)\n",
    "\n",
    "    pipeline = Pipeline(stages=[vectorAssembler, cv])\n",
    "    pipelineModel = pipeline.fit(train_df)\n",
    "    predictions = pipelineModel.transform(test_df)\n",
    "    r2 = evaluator.evaluate(predictions)\n",
    "\n",
    "    return pipelineModel, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3fe549a3-1ebc-42e2-b3f7-ed28358eb88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rf(train_df, test_df):\n",
    "    model = RandomForestRegressor(labelCol='Item_Outlet_Sales')\n",
    "\n",
    "    # Define an evaluation metric.  The CrossValidator compares the true labels with predicted values for each combination of parameters, and calculates this value to determine the best model.\n",
    "    evaluator = RegressionEvaluator(metricName=\"r2\",\n",
    "                                    labelCol=model.getLabelCol(),\n",
    "                                    predictionCol=model.getPredictionCol())\n",
    "    model = Pipeline(stages=[model])\n",
    "    model = model.fit(train_df)\n",
    "    predictions = model.transform(test_df)\n",
    "    r2 = evaluator.evaluate(predictions)\n",
    "\n",
    "    return model, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91c1d182-0582-4d7d-b23b-eea912ec44e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xgb(train_df, test_df):\n",
    "    model = SparkXGBRegressor(num_workers=2, label_col=\"Item_Outlet_Sales\", missing=0.0)\n",
    "\n",
    "    # Define an evaluation metric.  The CrossValidator compares the true labels with predicted values for each combination of parameters, and calculates this value to determine the best model.\n",
    "    evaluator = RegressionEvaluator(metricName=\"r2\",\n",
    "                                    labelCol=model.getLabelCol(),\n",
    "                                    predictionCol=model.getPredictionCol())\n",
    "    model = Pipeline(stages=[model])\n",
    "    model = model.fit(train_df)\n",
    "    predictions = model.transform(test_df)\n",
    "    r2 = evaluator.evaluate(predictions)\n",
    "\n",
    "    return model, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "996jxCkheEex",
   "metadata": {
    "id": "996jxCkheEex"
   },
   "outputs": [],
   "source": [
    "def process_cluster(cluster_id, train_df, test_df):\n",
    "    train = train_df.filter(train_df['Cluster'] == cluster_id)\n",
    "    test = test_df.filter(test_df['Cluster'] == cluster_id)\n",
    "    train = train.drop('Cluster')\n",
    "    test = test.drop('Cluster')\n",
    "\n",
    "    # model_xgb, r2_xgb = get_best_param_xgb(train, test)\n",
    "    # model_rf, r2_rf = get_best_param_rf(train, test)\n",
    "    model_xgb, r2_xgb = train_xgb(train, test)\n",
    "    model_rf, r2_rf = train_rf(train, test)\n",
    "\n",
    "    if r2_xgb > r2_rf:\n",
    "        best_model, best_model_name = model_xgb, 'XGB'\n",
    "    else:\n",
    "        best_model, best_model_name = model_rf, 'RF'\n",
    "\n",
    "    print(f'Cluster: {cluster_id}  r2_xgb = {r2_xgb}  r2_rf = {r2_rf}')\n",
    "\n",
    "    best_model_path = f'Models/{best_model_name}{cluster_id}'\n",
    "    best_model.write().overwrite().save(best_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3df4051c-ff35-4e13-b557-350611b52d50",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3df4051c-ff35-4e13-b557-350611b52d50",
    "outputId": "0893ba20-e5c3-4c04-c5a0-ccb4d6645461"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/xgboost/sklearn.py:782: UserWarning: Loading a native XGBoost model with Scikit-Learn interface.\n",
      "  warnings.warn(\"Loading a native XGBoost model with Scikit-Learn interface.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster: 0  r2_xgb = 0.45471250196419566  r2_rf = 0.5060597795006203\n",
      "Cluster: 1  r2_xgb = 0.6592619939871984  r2_rf = 0.6731302853080248\n"
     ]
    }
   ],
   "source": [
    "filepath=\"notebooks/Train.csv\"\n",
    "train_df, test_df = split_dataset(filepath)\n",
    "train_df=preprocessing_pipeline(train_df, train=True)\n",
    "test_df=preprocessing_pipeline(test_df, train=False)\n",
    "clusters = sorted(train_df.select(\"Cluster\").distinct().rdd.flatMap(lambda x: x).collect())\n",
    "if os.path.isdir('Models'):\n",
    "    shutil.rmtree('Models')\n",
    "os.mkdir('Models')\n",
    "for cluster_id in clusters:\n",
    "    process_cluster(cluster_id, train_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15be0770-7277-41ee-8af4-d4cc3f62f8b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "045b1fd8-1e5e-443c-9cf7-6c60dd358468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/work/notebooks\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4d11d0-c787-4517-b74c-e83e6358d821",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
